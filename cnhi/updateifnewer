#!/bin/bash

SCRIPTDIR=$( (cd -P $(dirname $0) && pwd) )
COREDIR=$( dirname $SCRIPTDIR  )
corename=$( basename $SCRIPTDIR )
DATADIR=${SCRIPTDIR}/data
DROPBOX=/lib_content30/Metadata_dropbox/CNHI_metadata

# load the shared bash functions log, vlog and Verbose
. $COREDIR/common/outputfuncs.bash $COREDIR/common

verbose=
force=
test=
index=staging
while getopts :vfti: opt
 do
      case $opt in
          v) verbose=-v;;
          t) test=-t;;
          f) force=-f;;
          i) index=$OPTARG
      esac
done
shift $((OPTIND-1))

if [ "$force" == "-f" ] ; then
    Verbose "Forcing index rebuild with -f flag"
fi
if [ "$AWS_ACCESS_KEY_ID" == "" ] ; then
    Echo "environment variable AWS_ACCESS_KEY_ID must be defined"
    exit 1
fi
if [ "$AWS_SECRET_ACCESS_KEY" == "" ] ; then
    Echo "environment variable AWS_SECRET_ACCESS_KEY must be defined"
    exit 1
fi
#if [ "$AWS_DEFAULT_REGION" == "" ] ; then
#    Echo "environment variable AWS_DEFAULT_REGION must be defined"
#    exit 1
#fi
if [ "$AWS_REGION" == "" ] ; then
    Echo "environment variable AWS_REGION must be defined"
    exit 1
fi

latest_datafile=`find_newest_file_matching_pattern_under_directory $DROPBOX 'CNHI_*.txt'`

cmp -s "$latest_datafile" $DATADIR/${corename}_data_16.txt

new_data_file=$?

if [[ "$new_data_file" != 0 ]] ; then
    Verbose "Copy new datafile from Metadata_dropbox"
    cp "$latest_datafile" $DATADIR/${corename}_data_16.txt
    iconv -f UTF-16 -t UTF-8 $DATADIR/${corename}_data_16.txt | tr '\n' ' ' | tr '\r' '\n' | egrep "[0-9]" | sed -e 's/^ //' > $DATADIR/${corename}_data.txt
fi

latest_datafile=`find $DROPBOX -name 'CNHI_*.txt' | sort | tail -1`

cmp -s "$latest_datafile" $DATADIR/${corename}_data_16.txt

new_data_file=$?

if [[ "$new_data_file" != 0 ]] ; then
    Verbose "Copying newer datafile from Metadata Dropbox"
    cp "$latest_datafile" $DATADIR/${corename}_data_16.txt 2>&1 | vlog ""
    iconv -f UTF-16 -t UTF-8 $DATADIR/${corename}_data_16.txt | tr '\n' ' ' | tr '\r' '\n' | egrep "[0-9]" | sed -e 's/^ //' > $DATADIR/${corename}_data.txt
fi

for line in `cat $SCRIPTDIR/cores_to_process`
do
    year=`date "+%Y"`
    solrname=`echo $line | cut -d '|' -f1`
    s3bucket=`echo $line  | cut -d '|' -f2 | sed -e "s/2020/$year/"`
    s3deletebucket=`echo $line | cut -d '|' -f3 | sed -e "s/2020/$year/"`
    solrselecturl=`echo $line | cut -d '|' -f4`
    Verbose "----------------------------------------"

    if [[ "$index" =~ $solrname ]]; then
        Verbose "Updating solr index for ${solrname}"
        if [[ "$force" == "-f" ||  "$new_data_file" != 0 || ! -e $DATADIR/${corename}_add_doc_${solrname}${test}.xml || $DATADIR/${corename}_data_16.txt -nt $DATADIR/${corename}_add_doc_${solrname}${test}.xml || $spreadsheet_jar -nt $DATADIR/${corename}_add_doc_${solrname}${test}.xml ]] ; then

            Verbose "Take UTF-16 encoded spreadsheet and index it using the java program in the cnhi_build jar."
            classpath="$spreadsheet_jar:$solrmarc_jar:$solrmarc_log_jar"
            cat $DATADIR/${corename}_data_16.txt | java -cp $classpath org.solrmarc.mixin.ReadCNHIDataVirgo4 -16 > $DATADIR/${corename}_add_doc_${solrname}${test}.xml

            Verbose "Now figure out if any records need to be deleted"
            cat $DATADIR/${corename}_add_doc_${solrname}${test}.xml | egrep '"id"' | sed -e 's/[^>]*>//' -e 's/<.*$//' |
sort > $DATADIR/cur_ids_being_added.ids
            Verbose "$solrselecturl/select?fl=id&q=data_source_f%3A${corename}&rows=10000"
            curl -s "$solrselecturl/select?fl=id&q=data_source_f%3A${corename}&rows=10000" | egrep '"id":' | sed -e 's/.*":"//' -e 's/".*$//' | sort  > $DATADIR/cur_ids_in_solr.ids
            Verbose "Num in solr = "`cat $DATADIR/cur_ids_in_solr.ids | wc -l`
            diff $DATADIR/cur_ids_in_solr.ids $DATADIR/cur_ids_being_added.ids | egrep '<' | sed -e 's/< //' > $DATADIR/records_to_delete_${corename}_${solrname}${test}.ids

            num_to_post=`egrep '"id"' $DATADIR/${corename}_add_doc_${solrname}${test}.xml | wc -l`
            Verbose "Upload ${num_to_post} transformed records to ${solrname} S3 bucket"
            if [ "$test" == "-t" ]; then
                Echo "aws s3 cp $DATADIR/${corename}_add_doc_${solrname}.xml ${s3bucket}/${corename}_add_doc_${solrname}.xml"
            else
                aws s3 cp $DATADIR/${corename}_add_doc_${solrname}.xml ${s3bucket}/${corename}_add_doc_${solrname}.xml
            fi

            if [ -s $DATADIR/records_to_delete_${corename}_${solrname}${test}.ids ] ; then
                Verbose "Some records in virgo solr that are not in the new dump of all records"
                if [ "$test" == "-t" ]; then
                    Verbose "Would delete "`cat $DATADIR/records_to_delete_${corename}_${solrname}.ids | wc -l`" records"
                    Echo "aws s3 cp $DATADIR/records_to_delete_${corename}_${solrname}.ids ${s3deletebucket}/records_to_delete_${corename}_${solrname}.ids"
                else
                    Verbose "Deleting "`cat $DATADIR/records_to_delete_${corename}_${solrname}.ids | wc -l`" records"
                    aws s3 cp $DATADIR/records_to_delete_${corename}_${solrname}.ids ${s3deletebucket}/records_to_delete_${corename}_${solrname}.ids
                fi
            else
               Verbose "    No records need to be deleted"
            fi
        else
            Verbose "No need to update solr index for ${solrname}"
        fi
    else
        Verbose "Skipping update of solr index for ${solrname}"
    fi
done

Verbose "Done updating core ${corename}"
